<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5.3 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>


  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
 <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="section01.html"><i class="fa fa-check"></i>Section I:
              Introduction</a>

          <li class="chapter" data-level="1"><a href="chapter01.html"><i class="fa fa-check"></i>1: Introduction</a>
          </li>

          <ul>
            <li class="chapter" data-level="2"><a href="chapter0101.html"><i class="fa fa-check"></i>1.1:
                Terminology</a></li>
            <li class="chapter" data-level="2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2: Process of
                Training a Machine Learning Model</a></li>
            <li class="chapter" data-level="2"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3: Preventing
                Overfitting</a></li>
            <li class="chapter" data-level="2"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4: Code
                Conventions</a></li>
            <li class="chapter" data-level="2"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5: Datasets
                Used</a></li>
            <li class="chapter" data-level="2"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
              Feature Engineering</a>
          <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                Domain-Specific Feature Engineering </a></li>
            <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                References</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
              Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
              </a></li>
            <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                Recommendation</a></li>
            <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                Engineering Categorical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                Engineering Ordinal Features </a></li>
            <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                Engineering Numerical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
              Effect Feature Engineering</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                Interaction Plot</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
            </li>
            <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                Conclusion</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
              Feature Selection</a>
          <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
              Feature Selection</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                Feature Selection Methods</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                Method</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                Method</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
              Selection Concerning Modeling Techniques</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                Lasso, Ridge, and ElasticNet</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                Importance of Tree Models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
            </li>
            <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                Tree-Based Feature Importance for Linear Model</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                Linear Model Feature Importance for Tree Models</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                Regression</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
              Selection Using Metaheuristic Algorithms</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                Feature Selection</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                Algorithm</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                Annealing</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                Swarm Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
              Model Explanation</a>
          <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
              and Model Predictions to Layman</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                Explainable models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                Explanation Techniques</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
              Special Chapters</a>
          <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
              Engineering & Selection for Text Classification</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                Construction</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                Selection</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                Extraction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                Reduction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                References</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things That Can
              Give Additional Improvement</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                Hyperparameter Tuning</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3: Ensemble
                Learning</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4: Signal
                Processing</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                References</a></li>
          </ul>
          </li>
        </ul>

      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">5.3:</span> Putting Everything Together</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>After learning different methods of
                    feature engineering, let&nbsp;s now put our knowledge into practice and measure the
                    model performance. We will use both linear and tree-based nonlinear models for
                    benchmarking purposes. In linear, we will use linear and logistic regression.
                    For tree-based nonlinear, we will use Lightgbm, and Xgboost models.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>In some cases, linear models will
                    perform better than nonlinear models and vice versa. We will select the model
                    which gives the best performance and is easy to explain to a non-technical
                    audience. The results will be the first benchmark performance. We will try to
                    find models which perform better than the benchmark performance using methods
                    discussed in section III.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%'><a name="_heading=h.2dlolyb"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext'>5.3.1<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp; </span>Hotel Total
                      Room Booking<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Let&nbsp;s try to understand the hotel
                    total room prediction data. It is a regression problem and the total occupancy
                    for hotels for a specific check-in date is the dependent variable. We tried
                    Lightgbm, Xgboost, and linear regression. Lightgbm regression gave the best
                    performance. Figure 5.3.1 explains the performance of the Lightgbm tree model
                    on cross-validation test, validation, and external test data.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The average RMSE for the
                    cross-validation test and validation data together is 16.4. The average RMSE
                    for the external test data is 12.9. This model predicts the total number of
                    rooms that will be sold for a future check-in date, based on which the hotel
                    property manager will ascertain market demand and set the price for the unsold
                    rooms.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>For any prediction made for total occupancy,
                    RMSE is an indicator as to what extent predictions might have an error. RMSE of
                    16.4 means, predictions might be off by 16 extra rooms or 16 less rooms. For
                    the external test data, we can see that model is performing better at 12.9
                    RMSE. There are 3 issues in this model. The first issue with the model is that
                    RMSE is very different across different test sets. The second issue is that
                    within the external test data, RMSE is different for each cross-validation. The
                    third issue is high RMSE i.e., we will like to have a model which has the
                    lowest amount of RMSE possible. If we can get a model with a lower RMSE, it
                    will be easier to convince the stakeholders to use the model.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>These 3 issues make the model
                    unreliable to use. We will try to use feature selection to reduce the noise in
                    the model. We will discuss different methods for feature selection across
                    different chapters in section III.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image28.png" o:spid="_x0000_i1089" type="#_x0000_t75" style='width:508.5pt;
 height:270pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image059.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=678 height=360 src="images/image060.jpg" v:shapes="image28.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Figure 5.3.1 performance of Lightgbm
                    tree model on cross-validation test, validation, and external test data for hotel
                    total room booking prediction<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%'><a name="_heading=h.sqyw64"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext'>5.3.2<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp; </span>Hotel Booking
                      Cancellation<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Let&nbsp;s try to understand the model
                    performance for hotel booking cancellations data. It is a classification
                    problem and 1 means canceled and 0 means not canceled. We tried Lightgbm,
                    Xgboost, and logistic regression models for classification.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We are using the precision score for
                    ascertaining model performance. Overbooking is a phenomenon wherein hotels
                    sometimes sell more rooms than their available. As a result, a situation can
                    arise when more than one guest can come to the hotel on the date of check-in to
                    request their stay. On the other hand, many guests cancel their booking before
                    the check-in date. This might lead to another problem wherein hotels have an
                    unsold inventory of rooms.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>If we can develop a model that can
                    identify transactions most likely to be canceled, with a high degree of
                    precision, hotels can minimize the loss occurring due to unsold inventory and
                    increase profit by indulging in overbooking.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Xgboost had the highest precision,
                    followed by Lightgbm. Xgboost had lower recall than Lightgbm. Figure 5.3.2
                    explains the performance of the Xgboost tree model on cross-validation test,
                    validation, and external test data for hotel booking cancellation.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The Xgboost classifier has a few issues.
                    Firstly, the precision values are not above 0.9 in both the test data.
                    Secondly, the precision score varies across cross-validation samples. For
                    example, in the first cross-validation training data, external test data has
                    the lowest precision score and for the third cross-validation training data,
                    precision is the highest. Thirdly, although precision scores are very near to
                    each other, there is a difference of 0.04 in precision scores between external
                    test data and the combination of cross-validation test data and validation
                    data.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image8.png" o:spid="_x0000_i1088" type="#_x0000_t75" style='width:521.5pt;
 height:275pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image061.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=695 height=367 src="images/image062.jpg" v:shapes="image8.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Figure 5.3.2 performance of Xgboost
                    tree model on cross-validation test, validation, and external test data for
                    hotel booking cancellation.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We will try different methods of
                    feature selection discussed across different chapters in section III to help
                    resolve the noise issue and try to identify features that are high in the
                    signal.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%'><a name="_heading=h.3cqmetx"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext'>5.3.3<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp; </span>Car Sales<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Let&nbsp;s try to understand the model
                    performance for car sales data. It is a regression problem and the selling
                    price of the car is the dependent variable. We tried linear regression,
                    Lightgbm, and Xgboost regressors to model the car price. The dependent variable
                    has car prices for used cars in Indian rupees.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Lightgbm performed better than the
                    rest of the modeling techniques. Figure 5.3.3 explains the performance of
                    Lightgbm for the combination of cross-validation test and validation data, as
                    well as for external test data. RMSE for the linear model is unreasonably high.
                    For the Lightgbm model, for the combination of cross-validation test data, and
                    validation data set, RMSE came as 406737.9, and for the external test data,
                    RMSE came as 264851.9. This means while predicting the price of used cars, the
                    model can make an error on an average to the extent of a little more than
                    400000 Indian rupees. In the US Dollar to Indian rupees exchange rate of 1 USD
                    = 80 Indian rupees, the model can make an error to the extent of $5000. This is
                    a very high error margin for a model that is trying to predict the price of
                    used cars.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>There are two more problems.
                    Firstly, RMSE is not consistent across both test datasets. A good model should
                    be able to generalize well. In this situation, the model cannot generalize
                    equally on the test, validation, and external test data. Secondly, RMSE varies
                    hugely across different test data samples in cross-validation. This is even
                    true for external test data. Here we can see that model with the provided set
                    of features is unable to predict reliably on the external test data, at
                    different cross-validation.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image16.png" o:spid="_x0000_i1087" type="#_x0000_t75" style='width:498.5pt;
 height:256.5pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image063.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=665 height=342 src="images/image064.jpg" v:shapes="image16.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Figure 5.3.3 performance of Lightgbm
                    tree model on cross-validation test, validation, and external test data for
                    used car price prediction.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We will try noise reduction from
                    features with the help of feature selection techniques in the next section. We
                    will try to develop a model which can predict with smaller, and acceptable
                    RMSE.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%'><a name="_heading=h.1rvwp1q"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext'>5.3.4<span style='mso-tab-count:1'>&nbsp;&nbsp;&nbsp; </span>Coupon
                      Recommendation<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We used Lightgbm, Xgboost, and
                    Logistic regression models and checked precision, and recall. Lightgbm
                    performed the best and is presented in figure 5.3.4 for the coupon recommendation
                    dataset.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image9.png" o:spid="_x0000_i1086" type="#_x0000_t75" style='width:513pt;
 height:271pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image065.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=684 height=361 src="images/image066.jpg" v:shapes="image9.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Figure 5.3.4 performance of Lightgbm
                    tree model on cross-validation test, validation, and external test data for
                    coupon recommendation.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Precision for both the test and
                    validation is close to 70, whereas, for the external test data, it is 72.44.
                    Recall for both the datasets are 74.5, and 77.9 respectively. Neither the
                    precision nor recall are satisfactory for this model. Our goal for this dataset
                    will be to remove noise from the data so that we can improve both the precision
                    and recall.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>We will try to use feature selection
                    to find features that are high in signal and we will remove features that are
                    high in noise in the next section of this book.<o:p></o:p></span></p>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="chapter0502.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="chapter0504.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


   <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>