<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7.9 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>
  <link rel="prev" href="index.html" />
  <link rel="next" href="intro.html" />
  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    (function (i, s, o, g, r, a, m) {
      i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
        (i[r].q = i[r].q || []).push(arguments)
      }, i[r].l = 1 * new Date(); a = s.createElement(o),
        m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-110543840-1', 'https://christophm.github.io/interpretable-ml-book/', {
      'anonymizeIp': true
      , 'storage': 'none'
      , 'clientId': window.localStorage.getItem('ga_clientId')
    });
    ga(function (tracker) {
      window.localStorage.setItem('ga_clientId', tracker.get('clientId'));
    });
    ga('send', 'pageview');
  </script>

  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"></i>Preface by the Author</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="section01.html"><i class="fa fa-check"></i>Section 01:
              Introduction</a>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter01.html"><i class="fa fa-check"></i>1: Introduction</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter0101.html"><i class="fa fa-check"></i>1.1:
                  Terminology</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2: Process of
                  Training a Machine Learning Model</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3: Preventing
                  Overfitting</a></li>
              <li class="chapter" data-level="2.4"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4: Code
                  Conventions</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5: Datasets
                  Used</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                  References</a></li>
            </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section 02:
              Feature Engineering</a>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
                  Feature Engineering </a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                  Introduction</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                  Domain-Specific Feature Engineering </a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                  References</a></li>
            </ul>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
                  Engineering </a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                  Introduction</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
                </a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                  Recommendation</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                  Conclusion</a></li>
            </ul>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
                  Feature Engineering </a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                  Engineering Categorical Features</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                  Engineering Ordinal Features </a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                  Engineering Numerical Features</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                  Conclusion</a></li>
            </ul>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
                  Effect Feature Engineering</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                  Interaction Plot</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
              </li>
              <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                  Everything Together</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                  Conclusion</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                  References</a></li>
            </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section 03:
              Feature Selection</a>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
                  Feature Selection</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                  Introduction</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                  Feature Selection Methods</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                  Method</a></li>
              <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                  Method</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                  Everything Together</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                  Conclusion</a></li>
            </ul>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
                  Selection Concerning Modeling Techniques</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                  Lasso, Ridge, and ElasticNet</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                  Importance of Tree Models</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
              </li>
              <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                  Tree-Based Feature Importance for Linear Model</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                  Linear Model Feature Importance for Tree Models</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                  Regression</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
              </li>
              <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
              </li>
              <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                  Everything Together</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                  Conclusion</a></li>
            </ul>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
                  Selection Using Metaheuristic Algorithms</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                  Feature Selection</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                  Algorithm</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                  Annealing</a></li>
              <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                  Optimization</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                  Swarm Optimization</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                  Everything Together</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                  Conclusion</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0809.html"><i class="fa fa-check"></i>8.8:
                  References</a></li>
            </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section 04:
              Model Explanation</a>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
                  and Model Predictions to Layman</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                  Introduction</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                  Explainable models</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                  Explanation Techniques</a></li>
              <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                  Everything Together</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                  Conclusion</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                  References</a></li>
            </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section 05:
              Special Chapters</a>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
                  Engineering & Selection for Text Classification</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                  Introduction</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                  Construction</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                  Selection</a></li>
              <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                  Extraction</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                  Reduction</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                  Conclusion</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                  References</a></li>
            </ul>
            <ul>
              <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things That Can
                  Give Additional Improvement</a>
              </li>
            </ul>
            <ul>
              <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                  Introduction</a></li>
              <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                  Hyperparameter Tuning</a></li>
              <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3: Ensemble
                  Learning</a></li>
              <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4: Signal
                  Processing</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                  Conclusion</a></li>
              <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                  References</a></li>
            </ul>
          </li>
        </ul>

      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">7.9:</span> Putting
                  Everything Together</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>For the 4 datasets, we will process
                    all the feature selection methods discussed in the book. In this section, we
                    will present the best results obtained for the dataset, across modeling
                    techniques and feature selection methods in this chapter. The best performance
                    will be briefly compared against the previously obtained best performance in
                    previous chapters, we will be performing cross-validation. The methods
                    discussed in this chapter return the list of features for each
                    cross-validation. For ease of understanding, we will look at how many features
                    are common across all cross-validation from among the selected features.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>For the tree feature importance, we
                    have considered the top 90 percent of features. For linear regression, we have
                    kept the top 95 percent of features based on the beta coefficient.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%'><a name="_heading=h.319y80a"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                  color:windowtext'>7.9.1<span style='mso-tab-count:1'>��� </span>Hotel Total
                      Room Booking<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We tried different models and
                    feature selection methods. For Lightgbm regression, when used with the feature
                    importance for 90% of top features, gave the best performance of all the
                    methods. For cross-validation test, and validation data RMSE was observed to be
                    17.9, and 12.3 for external test data. The detailed results for each
                    cross-validation can be seen in figure 7.9.1.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p>&nbsp;</o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="image40.png" o:spid="_x0000_i1081" type="#_x0000_t75" style='width:513.5pt;
                   height:270pt;visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image075.png"
                    o:title=""/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=685 height=360 src="images/image076.jpg" v:shapes="image40.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 7.9.1 performance of Lightgbm
                    tree model with filter method for feature selection on cross-validation test,
                    validation, and external test data for hotel total room booking prediction<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>This is worse than the previous
                    results presented in chapter 6. In addition to this, the results are
                    inconsistent across different cross-validations and different test, and
                    validation sets. Hence, we will discard this method.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%'><a name="_heading=h.1gf8i83"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                  color:windowtext'>7.9.2<span style='mso-tab-count:1'>��� </span>Hotel Booking
                      Cancellation<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We tried different models and
                    feature selection methods. Of all the methods, the Xgboost classifier, when
                    used with Boruta, gave the best performance. For the cross-validation test, and
                    the validation data precision was recorded at 0.835, and 0.881 for external
                    test data. The detailed results for each cross-validation can be seen in figure
                    7.9.2.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>The results obtained are almost
                    similar to the results obtained in the previous chapter. Although the precision
                    improved very marginally, the recall worsened than previous levels. There is no
                    additional advantage in the results, as the model suffers from the same
                    inadequacies that the model in chapter 6 suffered from. In such a scenario, it
                    is up to the judgment of the analyst whether the solution should be accepted or
                    we should keep searching for other solutions. In our case, we will still like
                    to try feature selection using metaheuristics techniques in chapter 8.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="image44.png" o:spid="_x0000_i1080" type="#_x0000_t75" style='width:519pt;
                   height:273.5pt;visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image077.png"
                    o:title=""/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=692 height=365 src="images/image078.jpg" v:shapes="image44.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 7.9.2 performance of Xgboost
                    tree model with Boruta method for feature selection on cross-validation test,
                    validation, and external test data for hotel total booking cancellation
                    prediction<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%'><a name="_heading=h.40ew0vw"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                  color:windowtext'>7.9.3<span style='mso-tab-count:1'>��� </span>Car Sales<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Lasso regression performed the best
                    for car sales data. For cross-validation test and validation data, RMSE was
                    233161, whereas for the external test data it is 260101. It is better than the
                    results obtained in chapter 6.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 7.9.3 shows the model
                    performance across different cross-validations. For the Lasso feature
                    selection, results between external test data and other test and validation
                    data are smaller than previously achieved results. It still suffers from 2
                    issues. Firstly, the RMSE is still higher than acceptable limits. As 200000
                    Indian rupees is still a very high error margin. Also, RMSE is not very
                    consistent across all cross-validations. Hence, we might need more improvements
                    to find a model with an acceptable RMSE.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="image37.png" o:spid="_x0000_i1079" type="#_x0000_t75" style='width:532pt;
                   height:273.5pt;visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image079.png"
                    o:title=""/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=709 height=365 src="images/image080.jpg" v:shapes="image37.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>Figure 7.9.3 performance of the
                    Lasso regression model on cross-validation test, validation, and external test
                    data for used car price prediction.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
                  0cm;line-height:150%'><a name="_heading=h.2fk6b3p"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
                  color:windowtext'>7.9.4<span style='mso-tab-count:1'>��� </span>Coupon
                      Recommendation<o:p></o:p></span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We tried different models and
                    feature selection methods. Of all the methods, the Xgboost classifier, when
                    used with the feature importance for the top 90 percent of features method,
                    gave the best performance. For the cross-validation test, validation data
                    precision was recorded at 0.700, and 0.753 for external test data. This
                    explains that the results are worse than the previously recorded best performance
                    in chapter 6. In addition to this, recall worsened than previous results to a
                    small extent. The detailed results for each cross-validation for precision can
                    be seen in figure 7.9.4.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>We will like to try the
                    metaheuristics feature selection methods to see if these methods can bring any
                    improvements.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
                   id="image42.png" o:spid="_x0000_i1078" type="#_x0000_t75" style='width:517pt;
                   height:272.5pt;visibility:visible;mso-wrap-style:square'>
                   <v:imagedata src="images/image081.png"
                    o:title=""/>
                  </v:shape><![endif]-->
                    <![if !vml]><img border=0 width=689 height=363 src="images/image082.jpg" v:shapes="image42.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
                  mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
                  8.0pt;margin-left:0cm;line-height:150%'><a name="_heading=h.upglbi"></a><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
                  "Times New Roman"'>Figure 7.9.4 performance of Xgboost tree model with feature
                    importance feature selection for top 90 percent of features on cross-validation
                    test, validation, and external test data for coupon recommendation dataset.<o:p></o:p></span></p>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="intro.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
  <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "sharing": {
          "github": true,
          "facebook": false,
          "twitter": true,
          "linkedin": true,
          "weibo": false,
          "instapaper": false,
          "vk": false,
          "whatsapp": false,
          "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
        },
        "fontsettings": {
          "theme": "white",
          "family": "sans",
          "size": 2
        },
        "edit": {
          "link": "https://github.com/christophM/interpretable-ml-book/edit/master/manuscript/index.Rmd",
          "text": "Edit"
        },
        "history": {
          "link": null,
          "text": null
        },
        "view": {
          "link": null,
          "text": null
        },
        "download": null,
        "search": {
          "engine": "fuse",
          "options": null
        },
        "toc": {
          "collapse": "subsection"
        }
      });
    });
  </script>

  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>