<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10.4 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>


  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
 <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="introduction.html"><i class="fa fa-check"></i>Section I:
              Introduction</a>

          <li class="chapter" data-level="1"><a href="chapter01.html"><i class="fa fa-check"></i>1: Introduction</a>
          </li>

          <ul>
            <li class="chapter" data-level="2"><a href="terminology.html"><i class="fa fa-check"></i>1.1:
                Terminology</a></li>
            <li class="chapter" data-level="2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2: Process of
                Training a Machine Learning Model</a></li>
            <li class="chapter" data-level="2"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3: Preventing
                Overfitting</a></li>
            <li class="chapter" data-level="2"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4: Code
                Conventions</a></li>
            <li class="chapter" data-level="2"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5: Datasets
                Used</a></li>
            <li class="chapter" data-level="2"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
              Feature Engineering</a>
          <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                Domain-Specific Feature Engineering </a></li>
            <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                References</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
              Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
              </a></li>
            <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                Recommendation</a></li>
            <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                Engineering Categorical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                Engineering Ordinal Features </a></li>
            <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                Engineering Numerical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
              Effect Feature Engineering</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                Interaction Plot</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
            </li>
            <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                Conclusion</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
              Feature Selection</a>
          <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
              Feature Selection</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                Feature Selection Methods</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                Method</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                Method</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
              Selection Concerning Modeling Techniques</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                Lasso, Ridge, and ElasticNet</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                Importance of Tree Models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
            </li>
            <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                Tree-Based Feature Importance for Linear Model</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                Linear Model Feature Importance for Tree Models</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                Regression</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
              Selection Using Metaheuristic Algorithms</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                Feature Selection</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                Algorithm</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                Annealing</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                Swarm Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
              Model Explanation</a>
          <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
              and Model Predictions to Layman</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                Explainable models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                Explanation Techniques</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
              Special Chapters</a>
          <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
              Engineering & Selection for Text Classification</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                Construction</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                Selection</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                Extraction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                Reduction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                References</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things That Can
              Give Additional Improvement</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                Hyperparameter Tuning</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3: Ensemble
                Learning</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4: Signal
                Processing</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                References</a></li>
          </ul>
          </li>
        </ul>

      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">10.4:</span> Feature Extraction</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>In the case of text
                    classification, feature extraction is done by representing text documents into matrices,
                    which can then be used by machine learning algorithms as features to classify
                    labels. For traditional machine learning models, feature extraction is done
                    mostly by either a bag of words matrix or the TF-IDF matrix.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.2eclud0"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.4.1<span style='mso-tab-count:1'>&nbsp; </span>Bag of
                      Words</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>It is a scoring method in which the
                    count of individual words is used for presence and 0 for absence. Let us
                    consider example 10.4.1 below with 4 corpora.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Example 10.4.1<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Corpus 1: I am very happy.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Corpus 2: I just had an awesome
                    weekend.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Corpus 3: I just had lunch. You had
                    lunch?<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Corpus 4: Do you want chips?<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Unique words from these 4 corpora,
                    after removing punctuation marks are: 'am', 'an', 'awesome', 'chips', 'Do',
                    'had', 'happy', 'I', 'just', 'lunch', 'very', 'want', 'weekend', 'you'<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Bag of words vector will appear in
                    table 10.4.1.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image15.png" o:spid="_x0000_i1032" type="#_x0000_t75" style='width:240.5pt;
 height:204.5pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image174.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=321 height=273 src="images/image175.gif" v:shapes="image15.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Table 10.4.1: Bag of words vector<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>In the above table, the corpus is
                    present in the column and the words are present in the row. If a specific word
                    is present in the corpus, its count is presented in corresponding cells against
                    the word. If a word is not present in the corpus, 0 is presented in the
                    corresponding cell.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>For example, the word &nbsp;<i
                      style='mso-bidi-font-style:normal'>lunch</i>&nbsp; is
                    present in corpus 3. Hence
                    count 2 is present in the corresponding cell. The word &nbsp;<i style='mso-bidi-font-style:
normal'>am</i>&nbsp; is only present in corpus 1, for once. Hence the corresponding
                    cells have the value 1. Again, the word &nbsp;<i style='mso-bidi-font-style:normal'>am</i>&nbsp;,
                    is not present in any other corpus apart from corpus 1. Hence, in corpus 2, 3,
                    and 4, the value in the corresponding cells is 0.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.thw4kt"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.4.2<span style='mso-tab-count:1'>&nbsp; </span>Term
                      Frequency
                      Inverse Document Frequency</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>This is otherwise abbreviated as
                    TF-IDF. Term frequency and inverse document frequency of each word are
                    calculated and multiplied to obtain the TF-IDF score.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Term frequency (TF) is calculated by
                    dividing how frequently a term appears in a corpus, by the total number of terms
                    in the corpus.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>TF = Count of the number of times
                    the term &nbsp;t&nbsp; is present in the corpus) / Count all the terms in the corpus.<o:p></o:p>
                    </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Inverse document frequency is
                    calculated by taking the logarithm of the total number of the corpus by the
                    count of documents that has the term.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>IDF = log (Total number of documents
                    / Count(documents which have term &nbsp;t&nbsp;))<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>TF-IDF vector for the corpora in
                    example 10.4.1 will appear as per table 10.4.2<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image20.png" o:spid="_x0000_i1031" type="#_x0000_t75" style='width:259pt;
 height:217pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image176.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=345 height=289 src="images/image177.gif" v:shapes="image20.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Table 10.4.2: TF-IDF vector<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.3dhjn8m"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.4.3<span style='mso-tab-count:1'>&nbsp;
                      </span>Word2vec</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Word2Vec is a shallow neural network
                    that tries to understand the context of words <sup>[10]</sup>. In word2vec,
                    individual words are represented as one-hot vectors and are used for creating a
                    vector space. It has a hidden layer, which is a fully-connected dense layer.
                    The weights of the hidden layer are the word embeddings. The output layer
                    outputs probabilities based on the Softmax activation function for the target
                    words from the vocabulary. Such a network is a &quot;standard&quot; multinomial
                    (multi-class) classifier.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The objective function of word2vec
                    is to maximize the log of similarity between the vectors for words that appear
                    close together in the context and minimize the similarity of words that do not.
                    This is called the Softmax function.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image19.png" o:spid="_x0000_i1030" type="#_x0000_t75" style='width:261.5pt;
 height:69.5pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image178.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=349 height=93 src="images/image179.jpg" v:shapes="image19.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Since classes are actual words, the
                    number of neurons is huge. The Softmax function when applied to such a huge
                    output layer will be computationally expensive. To save costly computation of
                    the Softmax in the output layer, noise-contrastive estimation is used. This
                    converts the multinomial classification problem to a binary classification
                    problem.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image17.png" o:spid="_x0000_i1029" type="#_x0000_t75" style='width:412pt;
 height:46.5pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image180.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=549 height=62 src="images/image181.jpg" v:shapes="image17.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><i style='mso-bidi-font-style:normal'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'>C</span></i><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'> is the context word, and <i style='mso-bidi-font-style:normal'>W</i> is the
                    focus word. <span class=SpellE>V<sub>c</sub></span>
                    is the embedding of context words and <span class=SpellE>V<sub>w</sub></span>
                    is the embedding of focus words. <o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Given a pair of words, we will
                    predict the context target or not. For this, we will create positive and
                    negative samples. For example, if 'Orange' and 'juice' are positive, it will be
                    inferred that these appear together in the same context. Similarly, if
                    'Orange', and 'king' are negative, it will be inferred that both words do not
                    appear in the same context. Positive samples are extracted from the context
                    window whereas negative sample is drawn randomly from the dictionary of all
                    words. Depending on the size of the data, if the data is small, negative sample
                    size k is selected between 5&lt;&gt;20 and for large, k is between 2&lt;&gt;5.
                    Accordingly, word pairs are created consisting of the input word, surrounding
                    context word, and target label whether it is a positive sample or negative
                    sample.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>For backpropagation, two matrices of
                    the same size are created namely embedding and context. The number of rows in
                    the matrix is the size of the vocabulary in the corpus and the number of
                    columns is defined as the size of the embedding. The embedding matrix has input
                    word representation and the context matrix has representation from context
                    word. At the start of the training process, random values are initialized in
                    these matrices.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>Through the dot product of input
                    embedding with each of the context embeddings, we obtain similarities between
                    the input and context embeddings. Resultant dot product numbers are converted
                    into positive numbers ranging between 0 and 1 using the sigmoid function. The
                    prediction error is obtained for each pair of input words and context words by
                    subtracting sigmoid transformation from the actual label value of the positive
                    sample(1) and negative sample(0). The error so obtained is inserted in the
                    embedding layer for each of the words in input for all the combination pairs.
                    This process is repeated through the dataset, based on the number of epochs
                    defined. Finally, the context matrix is discarded and the embedding matrix is
                    used.<o:p></o:p></span></p>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="chapter1003.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="chapter1005.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


   <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>