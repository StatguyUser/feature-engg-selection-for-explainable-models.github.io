<!DOCTYPE html>
<html lang="" xml:lang="">

<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10.3 | Feature Engineering & Selection for Explainable Models A Second Course for Data Scientists
  </title>


  <script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
  <link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
  <link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
  <link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
  <script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
  <script src="libs/kePrint-0.0.1/kePrint.js"></script>
  <link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
 <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-13135504-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-13135504-1');
</script>
<!-- End Google tag (gtag.js) -->


  <link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
  <script src="javascript/cookieconsent.min.js"></script>
  <script>
    window.addEventListener("load", function () {
      window.cookieconsent.initialise({
        "palette": {
          "popup": {
            "background": "#000"
          },
          "button": {
            "background": "#f1d600"
          }
        },
        "position": "bottom-right",
        "content": {
          "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
        }
      })
    });
  </script>

  <style>
    #cta-button-desktop:hover,
    #cta-button-device:hover {
      background-color: #ffc266;
      border-color: #ffc266;
      box-shadow: none;
    }

    #cta-button-desktop,
    #cta-button-device {
      color: white;
      background-color: #ffa31a;
      text-shadow: 1px 1px 0 #444;
      text-decoration: none;
      border: 2px solid #ffa31a;
      border-radius: 10px;
      position: fixed;
      padding: 5px 10px;
      z-index: 10;
    }

    #cta-button-device {
      box-shadow: 0px 10px 10px -5px rgba(194, 180, 190, 1);
      display: none;
      right: 20px;
      bottom: 20px;
      font-size: 20px;
    }

    #cta-button-desktop {
      box-shadow: 0px 20px 20px -10px rgba(194, 180, 190, 1);
      display: display;
      padding: 8px 16px;
      right: 40px;
      bottom: 40px;
      font-size: 25px;
    }

    @media (max-width : 450px) {
      #cta-button-device {
        display: block;
      }

      #cta-button-desktop {
        display: none;
      }
    }
  </style>






  <link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>





  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">
    <div class="book-summary">
      <nav role="navigation">

        <ul class="summary">
          <li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i
                class="fa fa-check"></i>Summary</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a href="foreward.html"><i
                class="fa fa-check"></i>Foreward</a></li>
          <li class="chapter" data-level="1" data-path="preface-by-the-author.html"><a
              href="preface-by-the-author.html"><i class="fa fa-check"> </i>Preface</a></li>
          <li class="chapter" data-level="1" data-path="intro.html"><a href="before-we-start.html"><i
                class="fa fa-check"></i>Before we start</a></li>
          <li class="chapter" data-level="1"><a href="section01.html"><i class="fa fa-check"></i>Section I:
              Introduction</a>

          <li class="chapter" data-level="1"><a href="chapter01.html"><i class="fa fa-check"></i>1: Introduction</a>
          </li>

          <ul>
            <li class="chapter" data-level="2"><a href="chapter0101.html"><i class="fa fa-check"></i>1.1:
                Terminology</a></li>
            <li class="chapter" data-level="2"><a href="chapter0102.html"><i class="fa fa-check"></i>1.2: Process of
                Training a Machine Learning Model</a></li>
            <li class="chapter" data-level="2"><a href="chapter0103.html"><i class="fa fa-check"></i>1.3: Preventing
                Overfitting</a></li>
            <li class="chapter" data-level="2"><a href="chapter0104.html"><i class="fa fa-check"></i>1.4: Code
                Conventions</a></li>
            <li class="chapter" data-level="2"><a href="chapter0105.html"><i class="fa fa-check"></i>1.5: Datasets
                Used</a></li>
            <li class="chapter" data-level="2"><a href="chapter0106.html"><i class="fa fa-check"></i>1.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section02.html"><i class="fa fa-check"></i>Section II:
              Feature Engineering</a>
          <li class="chapter" data-level="1"><a href="chapter02.html"><i class="fa fa-check"></i>2: Domain Specific
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter02.html"><i class="fa fa-check"></i>2.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0202.html"><i class="fa fa-check"></i>2.2:
                Domain-Specific Feature Engineering </a></li>
            <li class="chapter" data-level="2"><a href="chapter0203.html"><i class="fa fa-check"></i>2.3:
                References</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter03.html"><i class="fa fa-check"></i>3: EDA Feature
              Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter03.html"><i class="fa fa-check"></i>3.1:
                Introduction</a></li>
            <li class="chapter" data-level="2"><a href="chapter0302.html"><i class="fa fa-check"></i>3.2: Car Sales
              </a></li>
            <li class="chapter" data-level="2"><a href="chapter0303.html"><i class="fa fa-check"></i>3.3: Coupon
                Recommendation</a></li>
            <li class="chapter" data-level="2"><a href="chapter0304.html"><i class="fa fa-check"></i>3.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter04.html"><i class="fa fa-check"></i>4: Higher Order
              Feature Engineering </a>
          </li>
          <ul>
            <li class="chapter" data-level="2"><a href="chapter0401.html"><i class="fa fa-check"></i>4.1:
                Engineering Categorical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0402.html"><i class="fa fa-check"></i>4.2:
                Engineering Ordinal Features </a></li>
            <li class="chapter" data-level="2"><a href="chapter0403.html"><i class="fa fa-check"></i>4.3:
                Engineering Numerical Features</a></li>
            <li class="chapter" data-level="2"><a href="chapter0404.html"><i class="fa fa-check"></i>4.4:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter05.html"><i class="fa fa-check"></i>5: Interaction
              Effect Feature Engineering</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0501.html"><i class="fa fa-check"></i>5.1:
                Interaction Plot</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0502.html"><i class="fa fa-check"></i>5.2: SHAP</a>
            </li>
            <li class="chapter" data-level="2.3"><a href="chapter0503.html"><i class="fa fa-check"></i>5.3: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0504.html"><i class="fa fa-check"></i>5.4:
                Conclusion</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0505.html"><i class="fa fa-check"></i>5.5:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section03.html"><i class="fa fa-check"></i>Section III:
              Feature Selection</a>
          <li class="chapter" data-level="1"><a href="chapter06.html"><i class="fa fa-check"></i>6: Fundamentals of
              Feature Selection</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter06.html"><i class="fa fa-check"></i>6.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0602.html"><i class="fa fa-check"></i>6.2: Different
                Feature Selection Methods</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0603.html"><i class="fa fa-check"></i>6.3: Filter
                Method</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0604.html"><i class="fa fa-check"></i>6.4: Wrapper
                Method</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0605.html"><i class="fa fa-check"></i>6.5: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0606.html"><i class="fa fa-check"></i>6.6:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter07.html"><i class="fa fa-check"></i>7: Feature
              Selection Concerning Modeling Techniques</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0701.html"><i class="fa fa-check"></i>7.1:
                Lasso, Ridge, and ElasticNet</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0702.html"><i class="fa fa-check"></i>7.2: Feature
                Importance of Tree Models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0703.html"><i class="fa fa-check"></i>7.3: Boruta</a>
            </li>
            <li class="chapter" data-level="2.4"><a href="chapter0704.html"><i class="fa fa-check"></i>7.4: Using
                Tree-Based Feature Importance for Linear Model</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0705.html"><i class="fa fa-check"></i>7.5: Using
                Linear Model Feature Importance for Tree Models</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0706.html"><i class="fa fa-check"></i>7.6: Linear
                Regression</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0707.html"><i class="fa fa-check"></i>7.7: SVM</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0708.html"><i class="fa fa-check"></i>7.8: PCA</a>
            </li>
            <li class="chapter" data-level="2.5"><a href="chapter0709.html"><i class="fa fa-check"></i>7.9: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0710.html"><i class="fa fa-check"></i>7.10:
                Conclusion</a></li>
          </ul>
          <li class="chapter" data-level="1"><a href="chapter08.html"><i class="fa fa-check"></i>8: Feature
              Selection Using Metaheuristic Algorithms</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter0801.html"><i class="fa fa-check"></i>8.1: Exhaustive
                Feature Selection</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0802.html"><i class="fa fa-check"></i>8.2: Genetic
                Algorithm</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0803.html"><i class="fa fa-check"></i>8.3: Simulated
                Annealing</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0804.html"><i class="fa fa-check"></i>8.4: Ant Colony
                Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0805.html"><i class="fa fa-check"></i>8.5: Particle
                Swarm Optimization</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0806.html"><i class="fa fa-check"></i>8.6: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0807.html"><i class="fa fa-check"></i>8.7:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0808.html"><i class="fa fa-check"></i>8.8:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section04.html"><i class="fa fa-check"></i>Section IV:
              Model Explanation</a>
          <li class="chapter" data-level="1"><a href="chapter09.html"><i class="fa fa-check"></i>9: Explaining Model
              and Model Predictions to Layman</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter09.html"><i class="fa fa-check"></i>9.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter0902.html"><i class="fa fa-check"></i>9.2:
                Explainable models</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter0903.html"><i class="fa fa-check"></i>9.3:
                Explanation Techniques</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter0904.html"><i class="fa fa-check"></i>9.4: Putting
                Everything Together</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0905.html"><i class="fa fa-check"></i>9.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter0906.html"><i class="fa fa-check"></i>9.6:
                References</a></li>
          </ul>
          </li>
          <li class="chapter" data-level="1"><a href="section05.html"><i class="fa fa-check"></i>Section V:
              Special Chapters</a>
          <li class="chapter" data-level="2"><a href="chapter10.html"><i class="fa fa-check"></i>10: Feature
              Engineering & Selection for Text Classification</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter10.html"><i class="fa fa-check"></i>10.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1002.html"><i class="fa fa-check"></i>10.2: Feature
                Construction</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1003.html"><i class="fa fa-check"></i>10.3: Feature
                Selection</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1004.html"><i class="fa fa-check"></i>10.4: Feature
                Extraction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1005.html"><i class="fa fa-check"></i>10.5: Feature
                Reduction</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1006.html"><i class="fa fa-check"></i>10.6:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1007.html"><i class="fa fa-check"></i>10.7:
                References</a></li>
          </ul>
          <li class="chapter" data-level="2"><a href="chapter11.html"><i class="fa fa-check"></i>11: Things That Can
              Give Additional Improvement</a>
          </li>
          <ul>
            <li class="chapter" data-level="2.1"><a href="chapter11.html"><i class="fa fa-check"></i>11.1:
                Introduction</a></li>
            <li class="chapter" data-level="2.2"><a href="chapter1102.html"><i class="fa fa-check"></i>11.2:
                Hyperparameter Tuning</a></li>
            <li class="chapter" data-level="2.3"><a href="chapter1103.html"><i class="fa fa-check"></i>11.3: Ensemble
                Learning</a></li>
            <li class="chapter" data-level="2.4"><a href="chapter1104.html"><i class="fa fa-check"></i>11.4: Signal
                Processing</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1105.html"><i class="fa fa-check"></i>11.5:
                Conclusion</a></li>
            <li class="chapter" data-level="2.5"><a href="chapter1106.html"><i class="fa fa-check"></i>11.6:
                References</a></li>
          </ul>
          </li>
        </ul>

      </nav>
    </div>
    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
              <div id="forward-by-the-author" class="section level1 hasAnchor" number="1">
                <h1><span class="header-section-number">10.3:</span> Feature Selection</h1>
                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>Feature selection in text
                    classification is applicable for all features, except meta-features. In the
                    case of text classification, feature selection can be done through 1) the
                    Filter method 2) the embedded method using document frequency with a bag of
                    words or TF-IDF 3) the genetic algorithm 4) Ensemble feature selection.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>The biggest problem with
                    text features is that the number of unique features is in the thousands. If you
                    move from unigram to bi-gram, the number of unique bi-gram features increases
                    manifold. Not every token is useful and many are useless noise. We need to be
                    highly discriminatory to select a handful list of such unique features.<o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.47hxl2r"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.1<span style='mso-tab-count:1'>&nbsp; </span>Filter
                      Method</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:windowtext;mso-color-alt:windowtext;background:white;mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>In the case of text
                    classification, feature selection using the filter method is performed first
                    and then passed into for feature extraction before being fed into the model.
                    They try to find out the least useful features. When done correctly, these
                    methods can help in reducing computation time and prevent overfitting.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.2mn7vak"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.1.1 Document
                      Frequency</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext;
background:white;mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>It is the number of times a
                    word or n-gram is present across all documents <sup>[3]</sup>, discarding
                    repetitions. It is different from word occurrence count. Word count is how many
                    times the specific word or n-gram appeared across all documents. It does not
                    discard repetitions.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>For example, if we are
                    analyzing 100 documents and the word 'Lion' is present 10 times in only 1
                    document and not present in any other document and there is another word
                    'Bear', which is present in 7 documents and out of those 7 documents, it is
                    present 4 times in a single document. Word frequency for both 'Lion' and 'Bear'
                    is 10. However, document frequency is 1 for 'Lion' and 7 for 'Bear'.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>If we have to choose
                    between word and document frequency, we should remove features based on
                    document frequency as it has more impact on feature vector representation. A
                    word can have a very high word frequency because it is mentioned in a document
                    many times, but a word that is mentioned in multiple documents but
                    comparatively less frequently within each document can have more bearing on
                    feature representation. The same applies to high-frequency words, as these have
                    a low degree of bearing in the feature vector.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>Words that have either very
                    low or very high document frequency have little bearing on feature
                    representation. We can remove these words. The same principle can be applied to
                    higher-order n-grams, syntactic n-grams, and taxonomy features.<o:p></o:p></span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.11si5id"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.1.2 Chi-Square</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:windowtext;mso-color-alt:windowtext;background:white;
mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>It is otherwise known as X<sup>2</sup>
                    statistic <sup>[3]</sup> and it measures the lack of independence between
                    term(t) and class(c). It has a natural value of zero if t and c are
                    independent. If it is higher, then the term is dependent. It is not reliable
                    for low-frequency terms<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>Using the two-way
                    contingency table of a term t and a category c, where A is the number of times
                    t and c co-occur, B is the number of times the t occurs without c, C is the
                    number of times c occurs without t, D is the number of times neither c nor t
                    occurs, and N is the total number of documents, below will be the formula to
                    calculate Chi-square statistic.</span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image83.png" o:spid="_x0000_i1038" type="#_x0000_t75" style='width:323.5pt;
 height:50pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image162.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=431 height=67 src="images/image163.gif" v:shapes="image83.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>
                    <o:p></o:p>
                  </span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.3ls5o66"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.1.3 Mutual
                      Information</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext;
background:white;mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>In this method, rare terms
                    will have a higher score than common terms <sup>[3]</sup>. For multi-class
                    categories, we will calculate the MI value for all categories and will take the
                    Max (MI) value across all categories at the word level. It is calculated using
                    the below formula</span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image87.png" o:spid="_x0000_i1037" type="#_x0000_t75" style='width:229.5pt;
 height:51pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image164.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=306 height=68 src="images/image165.gif" v:shapes="image87.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>
                    <o:p></o:p>
                  </span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.20xfydz"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.1.4 Proportional
                      Difference</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext;
background:white;mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>It calculates how close two
                    numbers are to becoming equal <sup>[5][6]</sup></span><span lang=EN
                    style='color:black;mso-color-alt:windowtext'>
                    <w:Sdt SdtTag="goog_rdk_1" ID="-1359886591"><span style='font-family:Cardo;mso-fareast-font-family:Cardo;
 mso-bidi-font-family:Cardo;color:#222222;background:white;mso-highlight:white'>.
                        It helps &#64257;nd unigrams that occur mostly in one class of documents or
                        the other. It helps &#64257;nd unigrams that occur mostly in one class of
                        documents or the other. The values of PD are restricted between &#8722;1 and
                        1. Values near &nbsp;1 indicate that a word occurs in approximately an equal number
                        of documents in all categories and a 1 indicates that a word occurs in the
                        documents of only one category.</span></w:Sdt>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>Below is the formula for
                    calculating the proportional difference<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;mso-no-proof:yes'><!--[if gte vml 1]><v:shape id="image76.png"
 o:spid="_x0000_i1036" type="#_x0000_t75" style='width:187pt;height:48pt;
 visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image166.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=249 height=64 src="images/image167.gif" v:shapes="image76.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'><span style='mso-spacerun:yes'>&nbsp;</span>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>For multi-class categories,
                    we will calculate the MI value for all categories and will take the Max (MI)
                    value across all categories at the word level</span><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;mso-no-proof:yes'><!--[if gte vml 1]><v:shape id="image82.png"
 o:spid="_x0000_i1035" type="#_x0000_t75" style='width:223.5pt;height:34pt;
 visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image168.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=298 height=45 src="images/image169.gif" v:shapes="image82.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>
                    <o:p></o:p>
                  </span></p>

                <h5 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.4kx3h1s"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.1.5 Information
                      Gain</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:windowtext;mso-color-alt:windowtext'>
                      <o:p></o:p>
                    </span></b></h5>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>It measures the number of
                    bits of information obtained for category prediction by knowing the presence or
                    absence of a term in a document <sup>[3][4]</sup>. It is calculated using the
                    below formula</span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image86.png" o:spid="_x0000_i1034" type="#_x0000_t75" style='width:332.5pt;
 height:79pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image170.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=443 height=105 src="images/image171.gif" v:shapes="image86.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>
                    <o:p></o:p>
                  </span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.302dr9l"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.2<span style='mso-tab-count:1'>&nbsp;
                      </span>Metaheuristics
                      Algorithms</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:windowtext;mso-color-alt:windowtext;background:white;
mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:black;mso-color-alt:windowtext'>The genetic algorithm technique has been
                    found to perform well for the support vector machine technique with conceptual
                    representation wordnet features and TF-IDF feature representation <sup>[7]</sup>.
                    Instead of randomly selecting several text tokens as features, we can instead
                    use a percentage of text tokens as features from the total number of words
                    <sup>[8]</sup>.</span><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman"'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>The Genetic algorithm can be used
                    for deciding which text token should be included or excluded from the feature
                    vector. Although metaheuristic methods do not perform well when the number of
                    features is very high, like in the case of text classification, it has still
                    been cited in numerous research papers.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman"'>There are similar mentions of other
                    metaheuristics feature selection algorithms for text classification. However,
                    these methods all have one limitation in common, i.e., they cannot perform well
                    on high dimensional data where the number of features is numerous. <o:p></o:p></span></p>

                <h4 style='margin-top:16.0pt;margin-right:0cm;margin-bottom:8.0pt;margin-left:
0cm;line-height:150%;background:white'><a name="_heading=h.1f7o1he"></a><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:black;mso-color-alt:windowtext'>10.3.3<span style='mso-tab-count:1'>&nbsp; </span>Ensemble
                      Feature
                      Selection</span></b><b><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:windowtext;mso-color-alt:windowtext;background:white;
mso-highlight:white'>
                      <o:p></o:p>
                    </span></b></h4>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>There are linear and non-linear models which can be
                    trained individually to identify the target variable. Each technique has its
                    strengths and weaknesses. One way to leverage the strength of different models
                    is to perform ensemble learning by training all the base learner models. We can
                    also leverage the strength of the different types of features such as n-grams
                    and feature extraction methods, such as TF-IDF or bag of words vector.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>On one hand, it can significantly improve classification
                    model results, on the other hand, it can make the entire process
                    computationally heavy for actual use. We might need heavy computing power for
                    predicting labels from base models on new data. To solve this, we can 1) reduce
                    the complexity of individual base models and 2) reduce the number of base
                    models. This can be done by performing feature selection at both base models
                    and ensemble models. This entire process can be jointly done through an
                    algorithm called ensemble feature selection <sup>[9]</sup>. Figure 10.3.3
                    details the original ensemble feature selection algorithm<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>
                    <o:p>&nbsp;</o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;mso-no-proof:yes'><!--[if gte vml 1]><v:shape
 id="image97.png" o:spid="_x0000_i1033" type="#_x0000_t75" style='width:370.5pt;
 height:308pt;visibility:visible;mso-wrap-style:square'>
 <v:imagedata src="images/image172.png"
  o:title=""/>
</v:shape><![endif]-->
                    <![if !vml]><img border=0 width=494 height=411 src="images/image173.gif" v:shapes="image97.png">
                    <![endif]>
                  </span><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>
                    <o:p></o:p>
                  </span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>Fig 10.3.3: Ensemble feature selection (Wang et. al. 2020)<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>This algorithm was created for structured data. For text
                    classification, we can make adaptations at different levels. For the base
                    model, we can use document frequency to reduce words from the lower and higher
                    count of occurrence, instead of the lasso feature selection used in the
                    original paper. Through grid search, we can find the optimal combination of
                    lower and higher document frequency. For lower frequency, we use document count
                    and for the upper frequency, we use occurrence in percent of documents. By
                    reducing the number of words, feature vector size decreases and model
                    complexity also reduces.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>The original paper performs bootstrapping. In the
                    adaption, we are performing 5 or 10 cross-validations to avoid overfitting. The
                    entire labeled dataset can be divided into 3 parts 1) training data for
                    training individual models, 2) ensemble training data for training ensemble
                    model and 3) test data. This can be done in a 60:20:20 ratio. For each
                    cross-validation set, training data is used for training base learner models
                    such as logistic regression, random forest, etc. After training is complete,
                    the base model is used for predicting raw class probabilities for ensemble
                    training data and test data. An ensemble learning classifier is trained using
                    raw class probabilities for ensemble training data as features and actual
                    labels as the outcome variable. Finally, the ensemble learning model is used
                    for predicting outcomes for test data. A performance metric, such as the f1
                    score is used for each cross-validation and is averaged across all
                    cross-validations to ascertain model performance.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%'><span lang=EN style='font-family:"Times New Roman",serif;
mso-fareast-font-family:"Times New Roman";color:#222222;background:white;
mso-highlight:white'>For the ensemble model, we perform feature selection
                    through the genetic algorithm. This step is done as it is in the original
                    algorithm. Features that were discarded by the genetic algorithm are mapped
                    with all the base models. For any base model, if no class probability feature
                    is present in the list of features selected by the genetic algorithm, we can
                    discard those models. This entire process will result in fewer base models,
                    with each base model being less complex.<o:p></o:p></span></p>

                <p class=MsoNormal style='margin-top:16.0pt;margin-right:0cm;margin-bottom:
8.0pt;margin-left:0cm;line-height:150%;background:white'><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:#222222;background:white;mso-highlight:white'>Python implementation of
                    these filter methods, metaheuristics, and ensemble feature selection exists in
                    the companion python library </span><span class=SpellE><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:"Times New Roman";
color:white;background:#333333'>TextFeatureSelection</span></span><span lang=EN style='font-family:"Times New Roman",serif;mso-fareast-font-family:
"Times New Roman";color:#222222;background:white;mso-highlight:white'>.<o:p></o:p></span></p>
              </div>
            </section>

          </div>
        </div>
      </div>
      <a href="chapter1002.html" class="navigation navigation-prev " aria-label="Previous page"><i
          class="fa fa-angle-left"></i></a>
      <a href="chapter1004.html" class="navigation navigation-next " aria-label="Next page"><i
          class="fa fa-angle-right"></i></a>
    </div>
  </div>
  <script src="libs/gitbook-2.6.7/js/app.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
  <script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
  <script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>


   <script>
    gitbook.require(["gitbook"], function (gitbook) {
      gitbook.start({
        "search": {
          "engine": "fuse",
          "options": null
        },
        "info": false
      });
    });
  </script>
  <!-- dynamically load mathjax for compatibility with self-contained -->
  <script>
    (function () {
      var script = document.createElement("script");
      script.type = "text/javascript";
      var src = "true";
      if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
      if (location.protocol !== "file:")
        if (/^https?:/.test(src))
          src = src.replace(/^https?:/, '');
      script.src = src;
      document.getElementsByTagName("head")[0].appendChild(script);
    })();
  </script>
</body>

</html>